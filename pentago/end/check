#!/usr/bin/env python
# Verify the results of an MPI run against old out-of-core data

from __future__ import division
from pentago import *
from geode import *
from geode.utility import Log
from geode.value import parser
import hashlib
import glob
import sys
import re

# Parse properties
props = PropManager()
old_dir = props.add('old','../old-data-15july2012')
dirs = props.add('dir',None).set_required(1).set_hidden(1)
dirs.set([])
pad_io = props.add('pad_io',pentago_end_pad_io).set_help('whether or not to expect padding when checking sha1 hashes')
restart = props.add('restart',False)
reader_test = props.add('reader_test',-1) \
  .set_help('check consistency of given slice with smaller slices, using a reader_block_cache')
high_test = props.add('high_test',-1).set_help('check consistency of high level interface up to the given slice')
parser.parse(props,'Compare MPI results with out-of-core results',positional=[dirs])
old_dir = old_dir()

# Configure logging and threads
Log.configure('check mpi',False,False,100)
init_threads(-1,-1)
Log.write('dirs = %s'%' '.join(dirs()))
for dir in dirs():
  dir = os.path.normpath(dir)
  with Log.scope('dir %s'%dir):
    # Check whether the data is meaningless
    meaningless = 0
    for f in glob.glob(dir+'/meaningless-*'):
      assert not meaningless
      meaningless = int(f.split('-')[-1])

    if reader_test()>=0:
      # Load given slice and treat as ground truth
      cache_file = '%s/slice-%d.pentago'%(dir,reader_test())
      Log.write('loading reader test data from %s'%cache_file)
      cache = reader_block_cache(open_supertensors(cache_file),1<<30)
      set_block_cache(cache)
    else:
      # Generate meaningless data
      if meaningless:
        sections = sections_t(meaningless,all_boards_sections(meaningless,8))
        partition = simple_partition_t(1,sections,False)
        with Log.scope('meaningless'):
          store = compacting_store_t(estimate_block_heap_size(partition,0))
          cache = store_block_cache(meaningless_block_store(partition,0,0,store),1<<30)
        set_block_cache(cache)

    # Memorize the hashes of .pentago files from unit tests
    if meaningless:
      hashes = {r'empty.pentago':'0e6efa078eacb8dd20787f09ec9b4693355bfda3 2f500627ac2fe3f62d9670bfa3ce00cdd94537a8'
               ,r'meaningless-s4-r0/slice-0.pentago':'64ff1b2f49e25db61bf393c3de11ce74c7e52605 5b677b0778edb5b6703e84fc85c002ed6f8c8085'
               ,r'meaningless-s4-r17/slice-0.pentago':'64ff1b2f49e25db61bf393c3de11ce74c7e52605 7f2a056a7ca95129eaa1f2cc4fbd7cfefcab56bd'
               ,r'meaningless-s4-r\d+/slice-1.pentago':'00fd7a12b67bb05b891d15cda1682d5c072b93c1 bbdf25459d2475f46683514f7ad1ae34224f2f64'
               ,r'meaningless-s5-r0/slice-0.pentago':'66b2d8a3327879fcb75b51644e4160a2066c605f ae37f9af95ab20fd644cc890b2ababf7e20964ca'
               ,r'meaningless-s5-r17/slice-0.pentago':'66b2d8a3327879fcb75b51644e4160a2066c605f f5ec8c9c3425443a981a0d4f406ad64883b2c8eb'
               ,r'meaningless-s5-r\d+/slice-1.pentago':'aa0d47a7c827b8ff8de50c16700e333f99602a39 8c4ebf7867506c60ebd5d3f4a57ea7327d5c853d'
               ,r'meaningless-s4-r0/slice-2.pentago':'2a2e6bf617c8624c7fd6cb4a74ea42439102f329 e9f829447980b2d427bf96002332704c65c37015'
               ,r'meaningless-s4-r0/slice-3.pentago':'72a769dcf4fa47f881d4997785a0134e50b610aa db33d83c472e53fcf669e8c75bd277f8c5caecf7'
               ,r'meaningless-s4-r17/slice-2.pentago':'f54b5c1bea1261f01b75da2eb0a2748d78eb6b39 23bdbf069ecd68c01524a4b6075bcaba03613a9e'
               ,r'meaningless-s4-r17/slice-3.pentago':'7754a4235a373f5c5f00d78957e5caa996c27928 9b06ad92f6a9ec982944d93438f8b40cb3cf763f'
               ,r'meaningless-s5-r0/slice-2.pentago':'ec29c1cc2d72eb92a2d204dcd92c8e3d37121d50 6d1c0774631a5260b4ae2a22658f7f5b1036fc2e'
               ,r'meaningless-s5-r0/slice-3.pentago':'5c1bb0b5610cb8c46dec498b316aac7974ee055d f3ff4a5b2b78060c7d32d5c18b6f8ff9f6276f3d'
               ,r'meaningless-s5-r0/slice-4.pentago':'e0d48b631d3f12a61d33fea9822dc46ba70d8eb8 3f6d11649725a13b11774f889e181642ac63b41d'
               ,r'meaningless-s5-r17/slice-2.pentago':'a9a6c589b4a58a5e773c62bc3d993a3000417c69 6c0438587aaac9a7d64815f9d629f1efeaa86183'
               ,r'meaningless-s5-r17/slice-3.pentago':'3e1b9ab9bbd75ba9d16fd95b303b16a6fe413d24 539650866f5e742555b74cc39553b4b181220245'
               ,r'meaningless-s5-r17/slice-4.pentago':'0a948c604ba9cc2d356ec125fc3d2131552eb478 5bfb025e5729d39dc7785f75baae8099510656cc'
               ,r'write-3/slice-3.pentago':'4241e562b57deecd6a948f682719f817e7cb44f2 f12bc166507f4b31bf877f41583a989d32b3876c'
               ,r'write-4/slice-4.pentago':'c42d21eaa6d881a7ef7c496e5ddbccc2f756d597 bae84659c091f92add4f3fa7725210fb0a0212eb'}
      hashes = [(re.compile('.*/%s$'%k),v) for k,v in hashes.items()]
      def check_hash(file):
        for p,h in hashes:
          if p.match(file.replace('-restarted','')):
            h2 = hashlib.sha1(open(file).read()).hexdigest()
            Log.write('sha1sum %s = %s'%(os.path.basename(file),h2))
            h = h.split()[pad_io()]
            if h != h2:
              raise ValueError('%s: expected sha1sum %s, got %s'%(file,h,h2))
            break
        else:
          raise ValueError("don't know correct sha1sum for %s"%file)
    else:
      def check_hash(file):
        pass

    def to_native(x):
      return x.astype(x.dtype.newbyteorder('='))

    # Prepare to read data for high level test if necessary
    if high_test()>=0:
      high_data = reader_block_cache([s for f in glob.glob(dir+'/slice-*.pentago') for s in open_supertensors(f)],1<<30)

    # Process each slice that exists, keeping track of which files we've checked
    skip_pattern = re.compile('^(?:log(?:-\d+)?|empty.pentago|history.*|output-.*|.*\.pbs|meaningless-\d|\d+\.(cobaltlog|error|output))$')
    unchecked = set(os.path.join(dir,f) for f in os.listdir(dir) if not skip_pattern.match(f))
    random = Random(8183131)
    init_supertable(16)
    for slice in arange(36)[::-1]:
      empty_file = '%s/empty.pentago'%dir
      counts_file = '%s/counts-%d.npy'%(dir,slice)
      sparse_file = '%s/sparse-%d.npy'%(dir,slice)
      slice_file = '%s/slice-%d.pentago'%(dir,slice)
      restart_file = '%s/slice-%d-restart.pentago'%(dir,slice)
      if os.path.exists(counts_file):
        with Log.scope('check slice %d'%slice):
          # Read sparse samples
          samples = to_native(load(sparse_file))
          assert samples.shape[1]==9
          assert samples.dtype==uint64
          sample_boards = samples[:,0].copy()
          sample_wins = samples[:,1:].copy().reshape(-1,2,4)
          if sys.byteorder=='big':
            sample_wins = sample_wins[...,::-1].copy()
          unchecked.remove(sparse_file)

          # Read counts
          counts = to_native(load(counts_file))
          assert samples.shape[0]==256*counts.shape[0]
          assert counts.shape[1]==4
          assert counts.dtype==uint64
          sections = ascontiguousarray(counts[:,0].astype(counts.dtype.newbyteorder('<'))).view(uint8).reshape(-1,4,2)
          counts = counts[:,1:]
          Log.write('sections = %s'%' '.join(map(show_section,sections)))
          unchecked.remove(counts_file)

          # Compare against superengine
          with Log.scope('validity'):
            endgame_sparse_verify(sample_boards,sample_wins,random,len(sample_boards))

          # Run high level test if desired
          if slice<=high_test():
            with Log.scope('high level test'):
              high_board_t.sample_check(high_data,sample_boards,sample_wins)

          # Check empty.pentago hash
          check_hash(empty_file)

          # Open slice file
          if os.path.exists(slice_file):
            check_hash(slice_file)
            readers = open_supertensors(slice_file)
            expected_size = 20+3*4+sum(reader.total_size() for reader in readers)
            actual_size = os.stat(slice_file).st_size
            if pad_io():
              assert expected_size <= actual_size
            else:
              assert expected_size == actual_size
            assert len(counts)==len(readers)
            unchecked.remove(slice_file)

            # Check each section and block
            if reader_test()<0 or slice<reader_test():
              with Log.scope('consistency'):
                for reader,section,counts in zip(readers,sections,counts):
                  assert all(reader.header.section==section)
                  old_reader = None if meaningless or not old_dir else supertensor_reader_t('%s/section-%s.pentago'%(old_dir,show_section(section)))
                  counts2,samples_found = compare_readers_and_samples(reader,old_reader,sample_boards,sample_wins)
                  assert all(counts==counts2)
                  assert samples_found==256

            # Check restart consistency
            assert restart()==os.path.exists(restart_file)
            if restart():
              with Log.scope('restart'):
                restarts = open_supertensors(restart_file)
                unchecked.remove(restart_file)
                assert len(readers)==len(restarts)
                for read0,read1 in zip(readers,restarts):
                  assert all(read0.header.section==read1.header.section)
                  r0,r1,r2,r3 = map(xrange,read0.header.blocks)
                  for i0 in r0:
                    for i1 in r1:
                      for i2 in r2:
                        for i3 in r3:
                          b = i0,i1,i2,i3
                          assert all(read0.read_block(b)==read1.read_block(b))
          else:
            Log.write('WARNING: No slice file found, skipping consistency check for slice %d'%slice)

    # Yell if any files remain
    if unchecked:
      print 'strange files: %s'%' '.join(sorted(unchecked))
      assert False
Log.write('All tests passed!')
