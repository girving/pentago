#!/usr/bin/env python

from __future__ import division
from geode import *
from geode.value import parser
from pentago import *
import re

# Mimic options to endgame-mpi
props = PropManager()
ranks = props.add('ranks',0).set_required(True)
threads = props.add('threads',0).set_required(True)
block_size = props.add('block_size',8)
save = props.add('save',0).set_required(True)
dir = props.add('dir','data')
restart = props.add('restart',40).set_help('simulate restart from a given slice')
level = props.add('level',26)
memory = props.add('memory','').set_required(True)
section = props.add('section','').set_required(True)
samples = props.add('samples',128)
meaningless = props.add('meaningless',0)
randomize = props.add('randomize',0)
gather_limit = props.add('gather_limit',-1)
line_limit = props.add('line_limit',-1)
log_all = props.add('log_all',False)
per_rank_times = props.add('per_rank_times',False)
parser.parse(props,'Endgame prediction script',positional=[section])

# 'Parse' simple properties
ranks = ranks()
threads = threads()
block_size = block_size()
save = save()
samples = samples()
meaningless = meaningless()
restart = restart()

# Meaningless isn't supported by predict
if meaningless:
  raise ValueError('predict doesn\'t accept --meaningless')

# Parse memory
m = re.match('^(\d+(?:\.\d*)?|\.\d+)(M|MB|G|GB)$',memory())
if not m:
  raise ValueError('weird memory limit %s'%memory())
memory = int(float(m.group(1))*2**{'M':20,'G':30}[m.group(2)[0]])

# Parse section
m = re.match('^\D*(?:\d\d-)?(\d{8})\D*$',section())
if not m:
  raise ValueError('bad section %s'%section())
section = asarray(map(int,m.group(1))).reshape(4,2)

# Configure logging and threads
Log.configure('predict',0,0,100)
Log.write('command = %s'%parser.command(props))
init_threads(-1,0)

# Dump general information
with Log.scope('parameters'):
  Log.write('ranks = %d'%ranks)
  Log.write('cores = %d'%(ranks*threads))
  Log.write('threads / rank = %d'%threads)
  Log.write('section = %s'%show_section(section))
  Log.write('block size = %d'%block_size)
  Log.write('saved slices = %d'%save)
  Log.write('memory limit = %s'%large(memory))

line_cost = 2*64*8**3*420
def line_parallelism(base_memory):
  return (memory-base_memory)/line_cost
speed_estimate = 455092

# Limit the number of per rank blocks and lines for tag purposes
local_id_limit = 2**(14+6)-1
block_limit = 2**17

# Simulate execution
slices = descendent_sections(section,35)
prev_partition = None
prev_load = None
max_lines = 0
max_blocks = 0
max_local_ids = 0
max_base_memory = 0
min_lines = 1<<30
max_total_memory = 0
max_save_memory = 0
total_save_bytes = 0
total_outputs = total_inputs = 0
for slice in reversed(xrange(min(len(slices),restart+1))):
  if not len(slices[slice].sections):
    break
  with Log.scope('slice %d'%slice):
    if randomize():
      partition = random_partition_t(randomize(),ranks,slices[slice])
    else:
      partition = simple_partition_t(ranks,slices[slice],True)
    load = serial_load_balance(partition)
    with Log.scope('load balance'):
      Log.write(str(load))
    max_rank_lines = max(load.lines)
    max_rank_blocks = max(load.blocks)
    max_rank_nodes = max(load.block_nodes)
    Log.write('block local ids = %d (limit %d)'%(max(load.block_local_ids),local_id_limit))
    if max(load.block_local_ids) > local_id_limit:
      raise RuntimeError('max local ids = %d > %d'%(max(load.block_local_ids),local_id_limit))
    if max_rank_blocks > block_limit:
      raise RuntimeError('max rank blocks = %d > %d'%(max_rank_blocks,block_limit))
    if slice<restart:
      total_outputs += partition.sections.total_nodes
      total_inputs += prev_partition.sections.total_nodes if prev_partition else 0
      max_lines = max(max_lines,max_rank_lines)
      max_blocks = max(max_blocks,max_rank_blocks)
      max_local_ids = max(max_local_ids,max(load.block_local_ids))
      base_memory = max_rank_memory_usage(prev_partition,prev_load,partition,load)
      max_base_memory = max(max_base_memory,base_memory)
      Log.write('base memory = %s'%large(base_memory))
      lines = line_parallelism(base_memory)
      min_lines = min(min_lines,lines)
      Log.write('line parallelism = %g'%lines)
      total_memory = base_memory+line_cost*max(0,int(floor(lines)))
      max_total_memory = max(max_total_memory,total_memory)
      Log.write('total memory = %s'%large(total_memory))
    prev_partition = partition
    prev_load = load
    if slice<=save and slice<restart:
      save_memory = 64*max(load.block_nodes)
      Log.write('save memory = %s'%large(save_memory))
      max_save_memory = max(max_save_memory,save_memory)
      save_bytes = 64*partition.sections.total_nodes
      Log.write('save size = %s'%large(save_bytes))
      total_save_bytes += save_bytes
with Log.scope('summary'):
  Log.write('lines = %d'%max_lines)
  Log.write('blocks = %d'%max_blocks)
  Log.write('local ids = %d'%max_local_ids)
  Log.write('base memory = %s'%large(max_base_memory))
  Log.write('line parallelism = %g'%min_lines)
  Log.write('total memory = %s'%large(max_total_memory))
  Log.write('save memory = %s'%large(max_save_memory))
  Log.write('total inputs = %s'%large(total_inputs))
  Log.write('total outputs = %s'%large(total_outputs))
  Log.write('save bytes = %s'%large(total_save_bytes))
  Log.write('time estimate = %g s'%((total_outputs+total_inputs)/speed_estimate/ranks/threads))
