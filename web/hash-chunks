#!/usr/bin/env python
'''Compute md5 hashes of chunks of files'''

from __future__ import division,print_function,unicode_literals,absolute_import
from math import *
import multiprocessing.pool
import optparse
import hashlib
import sys
import os

# Options
usage = 'usage: %prog [options] [files...]\nCompute hashes of chunks of files'
parser = optparse.OptionParser(usage)
parser.add_option('--chunk-size',type=int,default=5*2**30-1,help='override chunk size (for debugging purposes only)')
parser.add_option('--np',type=int,default=6,help='number of processes')
parser.add_option('--block-size',type=int,default=16*2**20,help='block size (doesn\'t affect results)')
parser.add_option('--chunks',type=str,default='',help='hash a specific list of chunks')
options,paths = parser.parse_args()
if not paths:
  parser.error('at least one file require')
if options.chunks and len(paths)>1:
  parser.error('a specific list of chunks only makes sense with one file')

chunk_size = options.chunk_size
print('chunk size = %d'%chunk_size)

# Launch worker threads
pool = multiprocessing.pool.ThreadPool(options.np)

def hash_file(path):
  # Divide file into chunks
  name = os.path.basename(path)
  total_size = os.stat(path).st_size
  chunks = (total_size+chunk_size-1)//chunk_size
  digits = int(log10(chunks))+1
  print('\nname = %s'%name)
  print('size = %d'%total_size)
  print('chunks = %d'%chunks)
  print('digits = %d'%digits)

  # Hash each chunk
  def hash_chunk(c):
    f = open(path,'rb')
    f.seek(chunk_size*c)
    m = hashlib.md5() 
    left = min(total_size,chunk_size*(c+1))-chunk_size*c
    while left:
      n = min(left,options.block_size)
      data = f.read(n)
      assert len(data)==n
      m.update(data)
      left -= n
    print('%s  %s.%0*d'%(m.hexdigest(),name,digits,c+1))
    sys.stdout.flush()
  if not options.chunks:
    cs = range(chunks) 
  else:
    cs = [int(c)-1 for c in options.chunk.split(',')]
  pool.map(hash_chunk,cs)

for path in paths:
  hash_file(path)
