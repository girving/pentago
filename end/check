#!/usr/bin/env python
# Verify the results of an MPI run against old out-of-core data

from __future__ import division
from pentago import *
from other.core import *
from other.core.utility import Log
from other.core.value import parser
import hashlib
import glob
import sys
import re

# Parse properties
props = PropManager()
old_dir = props.add('old','../old-data-15july2012')
dirs = props.add('dir',None).set_required(1)
dirs.set([])
parser.parse(props,'Compare MPI results with out-of-core results',positional=[dirs])
old_dir = old_dir()

# Configure logging and threads
Log.configure('check mpi',False,False,100)
init_threads(-1,-1)
Log.write('dirs = %s'%' '.join(dirs()))
for dir in dirs():
  dir = os.path.normpath(dir)
  with Log.scope('dir %s'%dir):
    # Check whether the data is meaningless
    meaningless = 0
    for f in glob.glob(dir+'/meaningless-*'):
      assert not meaningless
      meaningless = int(f.split('-')[-1])

    # Generate meaningless data
    if meaningless:
      sections = sections_t(meaningless,all_boards_sections(meaningless,8))
      partition = simple_partition_t(1,sections,False)
      with Log.scope('meaningless'):
        store = compacting_store_t(estimate_block_heap_size(partition,0))
        cache = store_block_cache(meaningless_block_store(partition,0,0,store),1<<30)
      set_block_cache(cache)

    # Memorize the hashes of .pentago files from unit tests
    if meaningless:
      hashes = {r'empty.pentago':'0e6efa078eacb8dd20787f09ec9b4693355bfda3'
               ,r'meaningless-s4-r\d+/slice-0.pentago':'64ff1b2f49e25db61bf393c3de11ce74c7e52605'
               ,r'meaningless-s4-r\d+/slice-1.pentago':'00fd7a12b67bb05b891d15cda1682d5c072b93c1'
               ,r'meaningless-s5-r\d+/slice-0.pentago':'66b2d8a3327879fcb75b51644e4160a2066c605f'
               ,r'meaningless-s5-r\d+/slice-1.pentago':'aa0d47a7c827b8ff8de50c16700e333f99602a39'
               ,r'meaningless-s4-r0/slice-2.pentago':'2a2e6bf617c8624c7fd6cb4a74ea42439102f329'
               ,r'meaningless-s4-r0/slice-3.pentago':'72a769dcf4fa47f881d4997785a0134e50b610aa'
               ,r'meaningless-s4-r17/slice-2.pentago':'f54b5c1bea1261f01b75da2eb0a2748d78eb6b39'
               ,r'meaningless-s4-r17/slice-3.pentago':'7754a4235a373f5c5f00d78957e5caa996c27928'
               ,r'meaningless-s5-r0/slice-2.pentago':'ec29c1cc2d72eb92a2d204dcd92c8e3d37121d50'
               ,r'meaningless-s5-r0/slice-3.pentago':'5c1bb0b5610cb8c46dec498b316aac7974ee055d'
               ,r'meaningless-s5-r0/slice-4.pentago':'e0d48b631d3f12a61d33fea9822dc46ba70d8eb8'
               ,r'meaningless-s5-r17/slice-2.pentago':'a9a6c589b4a58a5e773c62bc3d993a3000417c69'
               ,r'meaningless-s5-r17/slice-3.pentago':'3e1b9ab9bbd75ba9d16fd95b303b16a6fe413d24'
               ,r'meaningless-s5-r17/slice-4.pentago':'0a948c604ba9cc2d356ec125fc3d2131552eb478'
               ,r'write-3/slice-3.pentago':'4241e562b57deecd6a948f682719f817e7cb44f2'
               ,r'write-4/slice-4.pentago':'c42d21eaa6d881a7ef7c496e5ddbccc2f756d597'}
      hashes = [(re.compile('.*/%s$'%k),v) for k,v in hashes.items()]
      def check_hash(file):
        for p,h in hashes:
          if p.match(file):
            h2 = hashlib.sha1(open(file).read()).hexdigest()
            Log.write('sha1sum %s = %s'%(os.path.basename(file),h2))
            if h != h2:
              raise ValueError('%s: expected sha1sum %s, got %s'%(file,h,h2))
            break
        else:
          raise ValueError("don't know correct sha1sum for %s"%file)
    else:
      def check_hash(file):
        pass

    def to_native(x):
      return x.astype(x.dtype.newbyteorder('='))

    # Process each slice that exists, keeping track of which files we've checked
    skip_pattern = re.compile('^(?:log(?:-\d+)?|empty.pentago|history.*|output-.*|meaningless-\d|\d+\.(cobaltlog|error|output))$')
    unchecked = set(os.path.join(dir,f) for f in os.listdir(dir) if not skip_pattern.match(f))
    random = Random(8183131)
    init_supertable(16)
    for slice in arange(36)[::-1]:
      empty_file = '%s/empty.pentago'%dir
      counts_file = '%s/counts-%d.npy'%(dir,slice)
      sparse_file = '%s/sparse-%d.npy'%(dir,slice)
      slice_file = '%s/slice-%d.pentago'%(dir,slice)
      if os.path.exists(counts_file):
        with Log.scope('check slice %d'%slice):
          # Read sparse samples
          samples = to_native(load(sparse_file))
          assert samples.shape[1]==9
          assert samples.dtype==uint64
          sample_boards = samples[:,0].copy()
          sample_wins = samples[:,1:].copy().reshape(-1,2,4)
          if sys.byteorder=='big':
            sample_wins = sample_wins[...,::-1].copy()
          unchecked.remove(sparse_file)

          # Read counts
          counts = to_native(load(counts_file))
          assert samples.shape[0]==256*counts.shape[0]
          assert counts.shape[1]==4
          assert counts.dtype==uint64
          sections = ascontiguousarray(counts[:,0].astype(counts.dtype.newbyteorder('<'))).view(uint8).reshape(-1,4,2)
          counts = counts[:,1:]
          Log.write('sections = %s'%' '.join(map(show_section,sections)))
          unchecked.remove(counts_file)

          # Compare against superengine
          with Log.scope('validity'):
            endgame_sparse_verify(sample_boards,sample_wins,random,len(sample_boards))

          # Check empty.pentago hash
          check_hash(empty_file)

          # Open slice file
          if os.path.exists(slice_file):
            check_hash(slice_file)
            readers = open_supertensors(slice_file)
            assert 20+3*4+sum(reader.total_size() for reader in readers)==os.stat(slice_file).st_size
            assert len(counts)==len(readers)
            unchecked.remove(slice_file)

            # Check each section and block
            with Log.scope('consistency'):
              for reader,section,counts in zip(readers,sections,counts):
                assert all(reader.header.section==section)
                old_reader = None if meaningless or not old_dir else supertensor_reader_t('%s/section-%s.pentago'%(old_dir,show_section(section)))
                counts2,samples_found = compare_readers_and_samples(reader,old_reader,sample_boards,sample_wins)
                assert all(counts==counts2)
                assert samples_found==256
          else:
            Log.write('WARNING: No slice file found, skipping consistency check for slice %d'%slice)

    # Yell if any files remain
    if unchecked:
      print 'strange files: %s'%' '.join(sorted(unchecked))
      assert False
Log.write('All tests passed!')
